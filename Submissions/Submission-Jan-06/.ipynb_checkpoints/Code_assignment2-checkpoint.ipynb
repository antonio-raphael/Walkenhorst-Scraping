{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8244831",
   "metadata": {},
   "source": [
    "# Assigment 2 - Preliminary Code\n",
    " **Subject** : Monopolistic behaviour assessment on food items in Californian prisons.\n",
    "\n",
    " **Authors** : Louise Gatty, Alice Pétillon, Charles Pyle, Antonio Raphael, Anne Thébaud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36eb0fc",
   "metadata": {},
   "source": [
    "## 1. Walkenhorts Family Visit Catalogue Scraping\n",
    "The following code in R (copy to R to run) takes the [Walkenhorst family visit pdf catalogue](https://assets.walkenhorsts.com/catalog_item/files/27/original/2025.pdf?1735933541) and performs pdf scraping creating a dataset of associated items and prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse) # basic functions\n",
    "library(conflicted) # force conflicts to errors\n",
    "library(pdftools) # read in pdf documents\n",
    "\n",
    "pdf <- pdf_text(\"~/Documents/PROJECT-CLONES/Data-Storage/Walkenhorst/family visit catalogue.pdf\") # read in data\n",
    "\n",
    "pdf <- pdf[-c(1,2)] # drop first two pages with no information\n",
    "\n",
    "PDF_Scraper <- function(page_number){\n",
    "  \n",
    "  pdf <- get(\"pdf\", parent.frame()) # get from global environment into sub-environment\n",
    "  \n",
    "  page <- pdf[[page_number]]# index by page number\n",
    "  \n",
    "  lines <- str_split(page, \"\\n\")[[1]] # split document on line breaks\n",
    "  \n",
    "  lines_df <- data.frame(lines) # convert to data frame\n",
    "  \n",
    "  lines_df <- lines_df[-c(1, 2, 67, 68),] # drop header and footer\n",
    "  \n",
    "  lines_df <- data.frame(lines_df) # re-covert to data frame\n",
    "  \n",
    "  lines_df <- lines_df |> \n",
    "    rename(V1 = lines_df) |> # rename\n",
    "    mutate(V1 = trimws(V1, which = \"left\", whitespace = \" \")) |> # remove extra whitespace\n",
    "    mutate(count = str_count(V1, \"\\\\b\\\\d{4,5}-\\\\d{3}\\\\b\")) |> # unneeded but leaving in so I dont have to change another line of code\n",
    "    mutate(position = str_locate_all(V1, \"\\\\b\\\\d{4,5}-\\\\d{3}\\\\b\")) |> # locate positions of item numbers\n",
    "    mutate(position = map(position, ~ sort(as.numeric(.x)[as.numeric(.x) >= 10]))) |> # drop initial item position\n",
    "    mutate(na=map_lgl(.x = position, .f = is_empty)) |> # test for empty positions where only one item\n",
    "    mutate(position_dollar = str_locate_all(V1, \"\\\\$\\\\d{1,2}\\\\.\\\\d{2}\")) |>  # dollar positions\n",
    "    mutate(position_dollar = map2(\n",
    "              position_dollar, na,\n",
    "              ~ if (.y == FALSE) character(0) else .x)) |> # dollar position empty if item position valid\n",
    "    mutate(break_code = map_chr(\n",
    "                      position,\n",
    "                      ~ if (length(.x) == 0) NA_character_ else .x[1] # take first position for item code break\n",
    "                    ),\n",
    "           break_dollar = map_chr(\n",
    "        position_dollar,\n",
    "        ~ if (length(.x) == 0) NA_character_ else .x[length(.x)] # take last position for dollar break\n",
    "      )\n",
    "    ) |> \n",
    "    mutate(break_code = as.numeric(break_code), \n",
    "           break_dollar = as.numeric(break_dollar) + 1) |> # convert to numeric and add one to dollar break position\n",
    "    mutate(break_point = coalesce(break_code,\n",
    "                                  break_dollar)) |> # coalesce to column of break points\n",
    "    select(-c(count:break_dollar)) |> # drop extra cols\n",
    "    mutate(first = str_sub(V1, 1, break_point-1), # first item\n",
    "           second = str_sub(V1, break_point, str_length(V1))) |> # second item\n",
    "    select(-c(V1, break_point))# drop extra data\n",
    "    \n",
    "  Full_Data <- data.frame(item = lines_df$first) |> # data frame as list of items, left hand column first\n",
    "    bind_rows(data.frame(item = lines_df$second)) # right hand column second\n",
    "  \n",
    "  Full_Data <- Full_Data |> \n",
    "    mutate(item = trimws(item, which = \"left\", whitespace = \" \")) |> # remove extra whitespace from left hand side\n",
    "    mutate(item_detect = str_detect(item, \"\\\\b\\\\d{4,5}-\\\\d{3}\\\\b\")) |> # detect item v item category\n",
    "    mutate(Category = case_when(item_detect == FALSE ~ item,\n",
    "                                TRUE ~ NA_character_)) |> # create column for walkenhorst categories\n",
    "    select(Category, item, item_detect) |> # reorder\n",
    "    fill(Category, .direction = \"down\") |> # fill category in down direction\n",
    "    dplyr::filter(item_detect == TRUE) |> # drop items with just categories\n",
    "    select(-c(item_detect)) |> # drop test\n",
    "    mutate(item_number = str_extract(item, \"\\\\b\\\\d{4,5}-\\\\d{3}\\\\b\"), # extract item number\n",
    "           item = str_remove(item, \"\\\\b\\\\d{4,5}-\\\\d{3}\\\\b\")) |> # remove item number from description\n",
    "    mutate(kosher = str_detect(item, \"ⓚ\"), # test if kosher\n",
    "           item = str_remove(item, \"ⓚ\")) |> # remove kosher symbol\n",
    "    mutate(price = str_extract(item, \"\\\\$\\\\d{1,2}\\\\.\\\\d{2}\"), # extract price\n",
    "           item = str_remove(item, \"\\\\$\\\\d{1,2}\\\\.\\\\d{2}\")) |> # remove price item description\n",
    "    mutate(weight = str_extract(item, \"\\\\d{1,3}\\\\.?(?:[^a-zA-Z]*?)oz\\\\.\"), # extract weight\n",
    "           weight2 = str_extract(item, \"\\\\d{1,3}\\\\.?(?:[^a-zA-Z]*?)oz\")) |> # extract weight different format\n",
    "    mutate(weight = coalesce(weight, weight2)) |> # coalesnce to one weight variable\n",
    "    select(-c(weight2)) |> # drop extraneous variable\n",
    "    mutate(item = trimws(item, which = \"both\", whitespace = \" \"))|> \n",
    "    mutate(item = trimws(item, which = \"both\", whitespace = \"[\\t\\r\\n]\")) # format to remove white space\n",
    "  \n",
    "  return(Full_Data) # return data\n",
    "}\n",
    "\n",
    "dataList <- list() # empty item list\n",
    "\n",
    "pages <- 1:31 # pages 1-31\n",
    "\n",
    "dataList <- lapply(pages, PDF_Scraper) # apply function to pages and save\n",
    "\n",
    "for (i in 2:31){\n",
    "  \n",
    "  dataList[[1]] <- dataList[[1]] |> \n",
    "    bind_rows(dataList[[i]]) # bind each page to first page\n",
    "\n",
    "}\n",
    "\n",
    "Full_Data <- dataList[[1]] # extact first page from list that now contains full data\n",
    "\n",
    "\n",
    "rm(list = ls(pattern = \"dataList|i|pages|pdf|PDF_Scraper\")) # drop unnecessary environmental variables\n",
    "\n",
    "\n",
    "Full_Data <- Full_Data |> \n",
    "  mutate(Category = trimws(Category, which = \"both\", whitespace = \"[\\t\\r\\n]\"),\n",
    "         Category = trimws(Category, which = \"both\", whitespace = \" \")) |> # clean category labels\n",
    "  fill(Category, .direction = \"down\") # fill in downward direction for categories that span multiple pages\n",
    "\n",
    "\n",
    "Full_Data <- Full_Data |> \n",
    "  mutate(weight = recode(weight,\n",
    "                         \"8.7” 12 oz.\" = \"12 oz.\")) |> \n",
    "  mutate(weight = str_remove_all(weight, \"oz.\"),\n",
    "         weight = str_remove_all(weight, \"oz\"),\n",
    "         weight = trimws(weight, which = \"right\", whitespace = \" \"),\n",
    "         weight = recode(weight,\n",
    "                         \"8.8.\" = \"8.8\",\n",
    "                         \"5 .\"  = \"5\",\n",
    "                         \"2 5/8\" = \"2.625\"),\n",
    "         weight = as.numeric(weight)) |> \n",
    "  mutate(weight = paste(weight, \" oz.\")) # clean weight vairable to consistent format and correct mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa166a4",
   "metadata": {},
   "source": [
    "## 2. Webscraping of Walmart items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45b53be",
   "metadata": {},
   "source": [
    "The following code in python takes the [Walmart website](https://www.walmart.com/search?q=crackers&page=2&affinityOverride=store_led) in California and webscrapes all the items for one category of products : \"crackers\". It uses asynchronous programming to enable multiple requests at once. \n",
    "\n",
    "For now, we have not been able to webscrape all categories associated to the pdf due to high antibot detection on Walmart's website, which potentially means we will need to use a VPN for the rest of the webscraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "\n",
    "import asyncio #allows multiple requests at once\n",
    "import json\n",
    "import math\n",
    "import httpx # makes the web requests\n",
    "import nest_asyncio\n",
    "import os\n",
    "from urllib.parse import urlencode\n",
    "from typing import List, Dict\n",
    "from loguru import logger as log\n",
    "from parsel import Selector #used to read HTML of the page and find the specific data\n",
    "from pathlib import Path\n",
    "\n",
    "#Fill these with the cookies and headers gotten by curl converter when giving HTLM curl link of request\n",
    "cookies = {} \n",
    "headers = {} \n",
    "\n",
    "nest_asyncio.apply() # allows to run asyncio inside Jupyter notebook\n",
    "\n",
    "# Function to extract search results from search HTML response\n",
    "def parse_search(html_text:str) -> tuple[list[dict], int]:\n",
    "    sel = Selector(text=html_text)\n",
    "    data = sel.xpath('//script[@id=\"__NEXT_DATA__\"]/text()').get() #script tag with JSON data\n",
    "    data = json.loads(data) # extract JSON data\n",
    "    total_results = data[\"props\"][\"pageProps\"][\"initialData\"][\"searchResult\"][\"itemStacks\"][0][\"count\"] #counts items\n",
    "    results = data[\"props\"][\"pageProps\"][\"initialData\"][\"searchResult\"][\"itemStacks\"][0][\"items\"] #gives items list\n",
    "    return results, total_results\n",
    "\n",
    "# Function to scrape a single Walmart search page\n",
    "async def scrape_walmart_page(session:httpx.AsyncClient, query:str=\"crackers\", page=1):\n",
    "    url = \"https://www.walmart.com/search?\" + urlencode({\"q\": query,\"page\": page,\"facet\": \"fulfillment_method:Pickup\",\"affinityOverride\": \"default\",},)\n",
    "    \n",
    "    for attempt in range(5): #tries up to 5 times to get a valid response\n",
    "        resp = await session.get(url)\n",
    "        if resp.status_code == 200:\n",
    "            return resp\n",
    "        await asyncio.sleep(2 ** attempt)  # exponential wait before retrying if it gets blocked (to mimic human behaviour)\n",
    "    raise Exception(f\"Blocked: {url}\")\n",
    "    resp = await session.get(url)\n",
    "    assert resp.status_code == 200, \"request is blocked\"\n",
    "    return resp \n",
    "\n",
    "# Function to determine number of pages for the request and scrape all search pages\n",
    "async def scrape_search(search_query:str, session:httpx.AsyncClient, max_scrape_pages:int=None) -> List[Dict]:\n",
    "    # scrape the first search page first\n",
    "    log.info(f\"scraping Walmart search for the keyword {search_query}\")\n",
    "    _resp_page1 = await scrape_walmart_page(query=search_query, session=session)\n",
    "    results, total_items = parse_search(_resp_page1.text) \n",
    "\n",
    "    # get the total number of pages available and the number of pages to scrape\n",
    "    max_page = math.ceil(total_items / 40)\n",
    "    if max_page > 25: # limits itself to 25 (Walmart's maximum limit for search results)\n",
    "        max_page = 25\n",
    "    if max_scrape_pages and max_scrape_pages < max_page:\n",
    "        max_page = max_scrape_pages\n",
    "    \n",
    "    log.info(f\"scraped the first search, remaining ({max_page-1}) more pages\")\n",
    "    # Asyncio.gather launches requests for all remaining pages at the same time\n",
    "    for response in await asyncio.gather(*[scrape_walmart_page(query=search_query, page=i, session=session) for i in range(2, max_page+1)]): \n",
    "        results.extend(parse_search(response.text)[0]) # it extends the list with results of each page\n",
    "    log.success(f\"scraped {len(results)} products from walmart search\")\n",
    "    return results \n",
    "\n",
    "# Main function to run the scraper and save results\n",
    "async def main():\n",
    "    export_dir = Path(\" To specify \") #path to export data\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)  #creates a folder \n",
    "\n",
    "    output_file = export_dir / \"walmart_crackers.json\"\n",
    "\n",
    "    #AsynClient Keeps the connection open and remebers cookiues. The rest runs the search fro \"crackers\"\n",
    "    async with httpx.AsyncClient(headers=headers,cookies=cookies, timeout=30) as session: results = await scrape_search(search_query=\"crackers\",session=session,max_scrape_pages=25)\n",
    "\n",
    "    #Saves everything to a JSON file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(results)} products to {output_file}\")\n",
    "\n",
    "#Runs the script\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b09d0d",
   "metadata": {},
   "source": [
    "Due to antibot blocks for some of us, we also used other libraries:\n",
    "- Thinking that standard httpx might be too \"obvious\" for Walmart some of us used **curl_cffi** instead of httpx. It is a library specifically designed to impersonate the TLS fingerprint of a real browser.\n",
    "- We also used **Selennium** to \"mimick\" human behaviour.\n",
    "\n",
    "We might need to use these when continuing the code on multiple queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
